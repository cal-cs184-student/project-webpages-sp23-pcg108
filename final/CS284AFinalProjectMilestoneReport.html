<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c11{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:234pt;border-top-color:#000000;border-bottom-style:solid}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.5pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.5pt;font-family:"Arial";font-style:italic}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.5pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12.5pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c8{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c18{border-spacing:0;border-collapse:collapse;margin-right:auto}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c12{color:inherit;text-decoration:inherit}.c9{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c5{height:11pt}.c2{height:0pt}.c3{background-color:#ffffff}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c3 c9 doc-content"><p class="c1"><span class="c3 c17">CS284A Final Project Milestone Report</span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c0 c3">Prashanth Ganesh, Joey Hou, Cheol Jun Cho, Cecil Symes</span></p><p class="c1"><span class="c3 c10">University of California, Berkeley, USA</span></p><p class="c1 c5"><span class="c3 c15"></span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c8"><a class="c12" href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1tFHLoALuu0P73iejeVewcm0PP4cWp_I89WGh3tjKPAg/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1682493594652019&amp;usg=AOvVaw1UUY9ERwL1oAQrPqJaHqXR">Progress Slides</a></span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c8"><a class="c12" href="https://www.google.com/url?q=https://drive.google.com/file/d/1b04V7dfv47VWNBSC9CqFxt9RjvvbFrjo/view&amp;sa=D&amp;source=editors&amp;ust=1682493594652570&amp;usg=AOvVaw0k8vYwnfntwiszRuxDquZr">Milestone Video</a></span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c13 c3">VR headsets provide an immersive way for users to interact with digital, virtual environments. However, rendering these complex 3D environments can be challenging for even the fastest hardware currently available, let alone anything more performance constrained. The Asynchronous Timewarp (ATW) is an operation that aims to reduce the perceived latency between the movement of the user and the displayed scene (motion-to-photon latency) by warping the generated frame to the updated position of the user at the time of display. Additionally, if a regular frame is unable to be rendered in time, then the ATW uses the last generated frame to make the display refresh deadline. In this project, we explore foveated rendering as a method to reduce the computation load required for ATW, thereby decreasing the latency of the system and increasing the perceived experience by the end user.</span></p><p class="c1 c5"><span class="c7 c3"></span></p><p class="c1 c5"><span class="c3 c7"></span></p><p class="c1"><span class="c6">INTRODUCTION</span></p><p class="c1"><span class="c0">The VR rendering pipeline consists of a frame first being rendered by the &ldquo;application&rdquo; (i.e. the virtual world generator running on the CPU and GPU, responsible for creating the scene and generating the images displayed on the screen), then read from the frame buffer and displayed on the VR headset. However, in the time it takes for a frame to be rendered and displayed from the frame buffer, the user&rsquo;s head orientation may have changed. This means that the displayed frame will be old, and slightly off from what should be displayed according to user motion. This slight difference interferes with the ideal user experience and precludes a true sense of presence in the virtual world. The Asynchronous Timewarp (ATW) [ 2] is a method for reducing this so-called motion-to-photon latency (the time it takes to emit photons to the screen from when the user turns their head). Not only can the timewarp use the current head position just ahead of the display refresh to generate an updated frame, it can also use prediction from the</span></p><p class="c1"><span class="c0">head tracking sensors to further improve the experience. The timewarp can be run asynchronously to the rendering of the original frame (hence the name) in order to further increase the perceived framerate. Specifically, the application engine can utilize the full capabilities of the graphics hardware without fear of missing a display refresh, because if that</span></p><p class="c1"><span>were to happen, the timewarp can use the last generated image to ensure that the deadline is met. As we can see, this operation should run as quickly as possible to optimally reduce motion-to-photon latency. Generally, the original frame </span><span class="c13 c3">is rendered at full high-quality resolution and the warp operation uses a high quality filter of the original frame. Our proposal is to use the foveated rendering technique (i.e. reducing the image quality in the periphery of human vision) to reduce the latency of the ATW.</span></p><p class="c1 c5"><span class="c13 c3"></span></p><p class="c1"><span class="c6 c3">METHODS</span></p><p class="c1"><span class="c13 c3">The goals of this project are reducing latency of asynchronous timewarp and providing more realistic, high quality rendering of VR scenes. To this end, We will develop foveated ATW on a testbed setting and conduct experiments with various hyperparameter configurations such as size of foveated regions, the magnitude of downsampling, and computing resources.</span></p><p class="c1 c5"><span class="c3 c13"></span></p><p class="c1"><span class="c4 c3">Foveated ATW</span></p><p class="c1"><span class="c13 c3">In the experiments, the eye gaze is assumed to move synchronously with the head movement and be fixed on the middle. The foveation is defined as zones where the zone at the middle will allow full-sampling and the zone at the peripheral will be sub-sampled. We will consider multiple configurations of defining zones, either in radial shape or square shape. The factor of sub-sampling is determined based on the distance of the zone from the middle, fixated point. We will test both exponentially or linearly decreasing sampling rates. The number of zones can be finite to be a few</span></p><p class="c1"><span class="c13 c3">levels like mipmap, or we can make the zone finer as continuous.</span></p><p class="c1 c5"><span class="c13 c3"></span></p><p class="c1"><span class="c4 c3">Environment</span></p><p class="c1"><span class="c13 c3">We use an open-source XR testbed, Illinois Extended Reality testbed (ILLIXR) [1 ]. This provides modular integration of state-of-the-art XR components. This work showed that the original version of this ATW algorithm misses the maximum per-frame execution time needed for the target frame rate on several benchmarks. Our goal then is to replace their ATW kernel with ours and demonstrate an improvement over their results. This involves installing ILLIXR on a desktop and integrating our algorithm into its flow. We set the environment on the desktop with Intel Xeon Silver 4314 (CPU) and NVIDIA RTX A5000 (GPU).</span></p><p class="c1 c5"><span class="c13 c3"></span></p><p class="c1"><span class="c3 c4">Benchmarks</span></p><p class="c1"><span class="c13 c3">The developed foveated ATW will be tested on a handful of commonly used VR benchmarks (Sponza, Materials, and Platformer) using the poses and camera/IMU data from Vicon Room 1 Medium dataset (this has been used in the prior work ILLIXR [1]).</span></p><p class="c1 c5"><span class="c13 c3"></span></p><p class="c1"><span class="c4 c3">Performance Metrics</span></p><p class="c1"><span class="c13 c3">It is critical for VR programs to maintain a high framerate in order to be convincing for the user, and prevent nausea. As such, the performance metrics will be directly related to the framerate of the program. The key metric will be the per-frame execution time, measured in milliseconds. This can then be used to calculate the frame-rate of the implementation, in frames per second (FPS), or Hertz. This can be compared head-to-head with the baseline implementation.</span></p><p class="c1 c5"><span class="c13 c3"></span></p><p class="c1 c5"><span class="c13 c3"></span></p><p class="c1"><span class="c6">PROGRESS</span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c19">Preliminary Results</span></p><p class="c1"><span class="c0">We successfully set up the environment to simulate XR rendering. We were able to figure out which part of the system we should modify to implement foveated ATW. As a first step, we implemented a new shader program to subsample the texture based on foveation. The images are in divided into three sections by three nested squares, where the dimension is 1/3x1/3, 2/3x2/3, and 1x1 (full), respectively. The factor of sampling rate is 1, 1/2, and 1/4 for the regions, where the 1 means full-sampling Fig. 1 (right). This configuration is arbitrary for the preliminary result. </span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.64px; height: 231.69px;"><img alt="" src="images/image2.png" style="width: 433.19px; height: 234.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 309.08px; height: 231.32px;"><img alt="" src="images/image1.png" style="width: 434.01px; height: 234.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c0">Figure 1 (default and foveated view)</span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c0">At this moment, we have not implemented interpolation and smoothing yet. This will be implemented within the shader. Preliminary benchmarking of ATW shows that the baseline algorithm takes 36 ms on average and the foveated ATW takes 41 ms on average. We aim to achieve more significant performance upgrades (as the interpolation step will likely increase the foveation latency). Therefore, for the next step, we are planning to implement the idea outside of the OpenGL shader, in the C++ backend. We have identified the part of the codes we can work on and have a strategy for it.</span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c19">Reflection</span></p><p class="c1"><span class="c0">The XR system is highly complex and setting the environment requires a lot of troubleshooting and code reviews. Environment set up up took more time than expected, but we expect to conduct all the experiments planned by the deadline.</span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c19">Plan</span></p><p class="c1"><span class="c0">Our planned timeline is as follows:</span></p><p class="c1 c5"><span class="c0"></span></p><a id="t.e91e0c27981dc31dbb81a6e8cb1affa64d1e9947"></a><a id="t.0"></a><table class="c18"><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c16"><span class="c0">Date</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c16"><span class="c0">Progress</span></p></td></tr><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Tuesday, 4/25</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Implement 4:1, 2:1, 1:1 sampling, identify data collection flow</span></p></td></tr><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Thursday, 4/27</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Finish initial implementation of foveation</span></p></td></tr><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Friday, 4/28</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Implement trilinear filtering, different foveation regions</span></p></td></tr><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Saturday, 4/29</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Complete data collection for baseline and foveation algorithms</span></p></td></tr><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Sunday, 4/30</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">(Time Permitting) Integrate eye tracking into foveated rendering</span></p></td></tr><tr class="c2"><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Tuesday, 5/2</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c1"><span class="c0">Complete and submit deliverables</span></p></td></tr></table><p class="c1 c5"><span class="c0"></span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c6">References</span></p><p class="c1 c5"><span class="c0"></span></p><p class="c1"><span class="c0">[1] Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V. Adve. 2021. ILLIXR: Enabling End-to-End Extended Reality Research. In 2021 IEEE International Symposium on Workload Characterization (IISWC). 24&ndash;38. https://doi.org/10.1109/IISWC53511.2021.00014</span></p><p class="c1"><span class="c0">[2] Johannes Marinus Paulus Van Waveren. 2016. The asynchronous time warp for virtual reality on consumer hardware. In Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology. 37&ndash;46.</span></p><p class="c1 c5"><span class="c0"></span></p></body></html>