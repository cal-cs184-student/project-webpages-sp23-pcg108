<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c15{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.5pt;font-family:"Times New Roman";font-style:normal}.c8{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c21{background-color:#ffffff;color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13.5pt;font-family:"Times New Roman";font-style:normal}.c2{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Times New Roman";font-style:normal}.c12{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Times New Roman";font-style:normal}.c0{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13.5pt;font-family:"Times New Roman";font-style:normal}.c11{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:13.5pt;font-family:"Times New Roman"}.c9{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c14{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12.5pt;font-family:"Times New Roman";font-style:normal}.c13{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c19{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c17{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c4{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c20{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c18{background-color:#ffffff;font-size:12pt;font-family:"Times New Roman";font-weight:400}.c22{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c7{color:inherit;text-decoration:inherit}.c16{font-size:21pt}.c3{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c22 doc-content"><p class="c17 title" id="h.hu0sktw9zfgp"><span class="c16">Foveated Rendering for Asynchronous Timewarp</span></p><p class="c19 subtitle" id="h.70baslm0ctas"><span>CS284A Final Project Report</span></p><p class="c5"><span class="c14">Prashanth Ganesh, Joey Hou, Cheol Jun Cho, Cecil Symes</span></p><p class="c5"><span class="c12">University of California, Berkeley, USA</span></p><p class="c1 c3"><span class="c12"></span></p><p class="c1 c3"><span class="c12"></span></p><p class="c1"><span class="c11"><a class="c7" href="https://www.google.com/url?q=https://drive.google.com/file/d/1qGMcvBqX08WL7JjKGQqW9_LWtUjuNfRv/view?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1683100208902393&amp;usg=AOvVaw1T47mKj9n49bucJjYcKrPu">Final Paper (ACM SIGGRAPH Style)</a></span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c11"><a class="c7" href="https://www.google.com/url?q=https://docs.google.com/presentation/d/14GjfBkQJqH_jBidTtfU5-OCB0EF08hpaPOG4zFWnNgc/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1683100208903114&amp;usg=AOvVaw02SCcaULY41b_cTZHFX6L9">Presentation Slides</a></span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c11"><a class="c7" href="https://www.google.com/url?q=https://drive.google.com/file/d/1iQqGGLsovJsKjA7Mjy8DNoR-ttfJib1J/view?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1683100208903679&amp;usg=AOvVaw16_QTJaZZJy_t2W-XPBpFv">Foveated ATW Clip</a></span></p><p class="c1 c3"><span class="c8"></span></p><p class="c1"><span class="c18 c20"><a class="c7" href="https://www.google.com/url?q=https://drive.google.com/file/d/1yTmSd-nlBbl2WtsQ0cY1XoxyXIi33swV/view&amp;sa=D&amp;source=editors&amp;ust=1683100208904278&amp;usg=AOvVaw3WMLSvzRo1zmDOlyvuL18I">Final Project Video</a></span></p><p class="c1 c3"><span class="c8"></span></p><p class="c1 c3"><span class="c8"></span></p><p class="c1"><span class="c18">Virtual Reality (VR) headsets offer an immersive way for users to interact with digital, virtual environments. However, rendering these complex 3D environments can be challenging for even the fastest hardware currently available, let alone on more performance constrained platforms. The Asynchronous Timewarp (ATW) is an operation that aims to reduce the perceived latency between the movement of the user and the displayed scene. It does so by warping the frame from displaying a position at the time of generation, to match the updated head rotation of the user at the time of display. Foveated rendering is a technique that takes advantage of human visual perception to optimize computing resources. It does so by maintaining image resolution in the area of the fovea and reducing resolution in the periphery. In this project, we explore applying the concept of foveated rendering within the mesh geometry used by the ATW transform, thereby decreasing the latency of the operation and improving the perceived experience by the end user. We demonstrate that our method achieves appreciable improvement over the baseline ATW algorithm while maintaining perceived image quality across a variety of benchmark scenes and foveation configurations.</span></p><p class="c1 c3"><span class="c12"></span></p><h1 class="c4" id="h.8hfxilozshnx"><span>INTRODUCTION</span></h1><p class="c1"><span class="c8">The VR rendering pipeline consists of a frame first being rendered by the &ldquo;application&rdquo; (i.e. the virtual world generator running on the CPU and GPU that is responsible for creating the scene and generating the images displayed on the screen). It is then read from the frame buffer, transformed to account for the distortion introduced by the curvature of the lens, and displayed on the VR headset. However, in the time it takes for a frame to be generated by the GPU and displayed from the frame buffer, the user&rsquo;s head orientation may change. This means that the generated frame no longer matches the scene that should be seen by the user in their new orientation. This difference interferes with the ideal user experience and precludes a true sense of presence in the virtual world.</span></p><p class="c1 c3"><span class="c8"></span></p><p class="c1"><span class="c8">The Asynchronous Timewarp (ATW) [3] is a method for reducing this so-called &quot;motion-to-photon&quot; latency (the delay between a user changing their pose and that change being reflected in the scene (Fig. 1 A)). The ATW samples (and/or predicts) the most updated head position just ahead of the display refresh deadline to transform the generated frame. Moreover, the ATW can be run asynchronously to the rendering of the original frame (hence the name) in order to further increase the perceived framerate. Specifically, the application engine can utilize the full capabilities of the graphics hardware without fear of missing a display refresh, because if that were to happen, the ATW can use the last generated image to ensure that the deadline is met. The ATW does all this while also applying barrel distortion to the scene to reverse the pincushion distortion introduced by the curvature of the lenses. As we can see, this operation must complete as fast as possible, because if the ATW misses the display refresh deadline, it results in the loss of the entire frame (which introduces judder). Typically, the original frame is rendered at full high-quality resolution and the ATW operation uses a high quality filter of the original frame to apply the barrel distortion and timewarp (Fig. 1 A). Our proposal is to use the foveated rendering concept (i.e. reducing the render resolution in the periphery of human vision) to reduce the latency of the ATW. We do so by progressively modifying the resolution of the mesh within the ATW, thereby decreasing the amount of computation done by the GPU to transform the scene to the updated head position and lens curvature (Fig. 1 C).</span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 361.33px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 361.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c3"><span class="c9"></span></p><h1 class="c4" id="h.u51zr4hz01aq"><span class="c10">METHODS</span></h1><p class="c1"><span class="c8">The classic ATW algorithm introduced by [3] operates by sampling the user&rsquo;s position at both the time of the frame generation as well as the time of the ATW operation. These two positions provide rotation matrices that are used to derive a transform that is then combined with barrel distortion, computed from the physical lens characteristics, to be applied to the most recent GPU-generated frame. The ATW does so by constructing a 2-D mesh from the pixels within the scene. Mesh points are selected at regular intervals of 32 pixels. The aforementioned combined transform is applied to this mesh to achieve a warped frame that is ready to be displayed on the screen. The implementation of this baseline ATW algorithm used in [ 1] considers the mesh points as the vertices of triangles (Fig. 2) that are used as inputs to an OpenGL vertex shader that applies the computed ATW transform described above.</span></p><p class="c1 c3"><span class="c9"></span></p><h2 class="c13" id="h.dir9xi3ixosu"><span class="c6">Foveated ATW</span></h2><p class="c1"><span class="c8">In our method of applying the concept of foveation to the ATW, we propose to subsample this mesh in accordance with defined foveation regions, thereby reducing the resolution of the mesh in regions that are further from the user&rsquo;s fovea. This effectively reduces the computation load of the timewarp transform described above and decreases the latency of the overall operation. In our implementation, the eye gaze is assumed to move synchronously with the head (i.e. fixed foveated rendering). The frame is split into 3 regions of decreasing mesh resolution, with the central region of the mesh (fovea) sampled at full resolution, the outer region at a lower resolution, and the outermost region at an even lower resolution. We experiment with a number of foveation region shapes (i.e. rectangular and radial) and sampling frequencies in the subsampled regions. In the context of the OpenGL vertex shader described earlier, this effectively means we are increasing the sizes of the triangles in the non-fovea regions of the mesh, which decreases the number of triangles in that region. An example of the mesh structure for the baseline implementation as well as two configurations in our algorithm is shown in Figure 2.</span></p><p class="c1 c3"><span class="c8"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 276.00px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1 c3"><span class="c9"></span></p><h2 class="c13" id="h.afm3l8mafbox"><span class="c6">Environment</span></h2><p class="c1"><span class="c8">We used the open-source XR testbed known as the Illinois Extended Reality testbed (ILLIXR) [1]. ILLIXR provides an end-to-end XR system with modular integration of state-of-the-art XR components, including the baseline ATW algorithm described earlier. This work showed that the baseline ATW algorithm exceeds the maximum per-frame execution time needed for the target frame rate on several benchmarks and platforms. Our goal is to replace their ATW kernel with our foveated version, and demonstrate an improvement over classic ATW in a number of metrics. We installed ILLIXR on a desktop with an Intel Xeon Silver 4314 (CPU) and NVIDIA RTX A5000 (GPU), and integrated our mesh foveation algorithm as a plugin to ILLIXR. Our plugin is implemented in C++ and links to the vertex shader written in the GLSL shader language (as taught in CS284A Project 4). Figure 1 illustrates the role of the ATW in the VR pipeline.</span></p><p class="c1 c3"><span class="c9"></span></p><h2 class="c13" id="h.71xwpyq0jdxj"><span class="c6">Benchmarks</span></h2><p class="c1"><span class="c8">We tested our implementation of foveated ATW with 3 different scenes of increasing difficulty from the EuRoC MAV Dataset (consisting of poses and camera/IMU data). The Easy scene (Scenario-1) has the least amount of movement and primarily consists of smooth travel. The Medium scene (Scenario-2) has more movement around the room, and the Difficult scene (Scenario-3) has the most movement alongside rapid changes in pose. There are three axes of defining foveation: shape of foveation, the size of fovea, and downsampling factors of peripheral regions. We considered rectangular and radial patterns of foveation (Fig. 2). The center of foveation is defined as a ratio of fovea size in the central 50% or 25% of the screen, wherein meshes are exhaustively sampled at every mesh point. The peripherals are divided into two equally sized regions. The downsampling factors are controlled to be 1:2 and 1:4, or 1:4 and 1:8, to investigate a difference in the magnitude of foveation. The resulting 8 combinations of configurations are organized in Table 1.</span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 284.00px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 284.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c3"><span class="c9"></span></p><h2 class="c13" id="h.67iavu1bqw94"><span class="c6">Performance Metrics</span></h2><p class="c1"><span class="c2">It is critical for VR programs to maintain a high framerate in order to be convincing for the user and prevent nausea. As such, our performance metrics are selected to reflect this. We measured average per-frame execution time (ms), latency per frame (ms), and stale frame ratio (the proportion of frames that missed the VSYNC deadline for a 30 Hz display and are thus considered &quot;stale&quot;).</span></p><p class="c1 c3"><span class="c2"></span></p><h1 class="c4" id="h.dbxofypkuta4"><span class="c10">RESULTS</span></h1><p class="c1"><span class="c8">Figures 3, 4, and 5 illustrate the average execution time per frame for each configuration, as well as the proportion of stale frames for each configuration across the 3 scenarios. Figure 6 illustrates the same frame across 4 different foveation configurations and highlights the artifacts introduced by the different foveation methods when compared to the baseline.</span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 298.67px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 298.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 684.95px; height: 412.83px;"><img alt="" src="images/image3.png" style="width: 684.95px; height: 412.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 695.44px; height: 384.50px;"><img alt="" src="images/image5.png" style="width: 695.44px; height: 384.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c4" id="h.a2cjlpexm5u6"><span>D</span><span class="c10">ISCUSSION</span></h1><h2 class="c13" id="h.dmrvcd4ub2mx"><span class="c6">Performance</span></h2><p class="c1"><span class="c8">Figure 3 illustrates the reduction in both average frame latency as well as the proportion of stale frames across all 3 scenarios and all 8 configurations. Across all scenarios, we can see that the most marked reduction in latency and stale frame proportion is seen in the configurations in which the fovea size is 25% of the frame and the sampling rates in the 2 outer regions are 1:4 and 1:8, respectively. This aligns with the observation that this configuration has the greatest reduction in the overall number of triangles processed by the vertex shader (since the vertex shader has to apply a matrix-vector operation to a given triangle for the timewarp+curvature transformation). Figures 4 and 5 show the per-frame latency for these two configurations. We can see that the foveated versions outperform the baseline consistently on nearly every frame. However, nearly all of the configurations also resulted in marked improvements in these metrics. As we will discuss in the following section, there is a balance between acceptable image quality and performance to be considered.</span></p><p class="c1 c3"><span class="c9"></span></p><h2 class="c13" id="h.xlf1jhs8afsb"><span class="c6">Visual Effects</span></h2><p class="c1"><span class="c8">In Figure 6, we can see a comparison of the same frame in Scenario-2 between the original ATW, foveated ATW with rectangular region shapes, and foveated ATW with radial region shapes with different region sizes and sample rates. Figure 2 shows the region and triangle distribution for Fig.6 B and Fig.6 D (the difference for Fig.6 A and Fig.6 C being a larger fovea and less sub-sampling in the outer regions). We can see that especially on the boundaries between regions, the coarseness of the triangles in the outer regions (as a result of decreased sample rates) results in a less smooth warping of the frame to the new pose and lens curvature. This is illustrated in sections where the image does not seem to flow smoothly and appears disconnected. However, we can clearly see that the frames in Fig.6 A and Fig.6 C have significantly more acceptable artifacts than Fig.6 B and Fig.6 D, at the expense of a few milliseconds of average frame latency (Figure 3). Thus, it is clear that our foveated ATW algorithm can offer significant performance gains without sacrificing too much in image quality.</span></p><p class="c1 c3"><span class="c9"></span></p><h2 class="c13" id="h.7ek1zch21zm"><span class="c6">Alternate Approaches</span></h2><p class="c1"><span class="c8">It is interesting to note that our initial approach focused on applying foveation to the fragment shader. We reduced the amount of texture sampling done within the fragment shader to only be at regularly spaced sample points within each foveation region, and used interpolation to compute intermediate pixel values. This effectively reduces the texture sampling load but not the ATW computation load, and aligns more closely with standard foveated rendering. However, we discovered that there was little to no per-frame latency reduction with this method. We realized as a result that the ATW fragment shader was not, in fact, a performance bottleneck to the overall pipeline. However, when we experimented with reducing the resolution of the mesh inputs to the vertex shader, that is where we found appreciable reductions in latency. This led us to our approach of modifying the mesh structure used in the ATW transform itself, which produced the improved results illustrated above.</span></p><p class="c1 c3"><span class="c8"></span></p><h1 class="c4" id="h.9yvkfeq8f7du"><span class="c10">FUTURE WORK</span></h1><h2 class="c13" id="h.qegghn5eno89"><span class="c6">Eye Tracking</span></h2><p class="c1"><span class="c8">Our current implementation assumes the eye is focused on the center of each frame. As such, the fovea is always placed in the center. If a user doesn&rsquo;t move their heads, and instead moves their eyes to glance at the outer regions of a given frame, the reduction in visual fidelity may be noticeable. The current implementation could be improved by moving the foveation region with the gaze of the user. To implement this, data would first need to be collected across various test subjects with a headset that supports eye tracking, to explore the coupling of head and eye movement. Studies such as [2] show a strong correlation between head tracking and eye gaze, but more data could provide further insight.</span></p><p class="c1 c3"><span class="c9"></span></p><h1 class="c4" id="h.3dzxdr3ubijg"><span class="c10">CONCLUSION</span></h1><p class="c1"><span class="c8">In our work, we propose and implement an algorithm by which the mesh structure used for the asynchronous timewarp operation can be optimized by using foveation. We demonstrate that our method reduces the amount of computation required to transform a given frame to an updated user head pose and lens curvature, resulting in a marked improvement in performance without sacrificing visual quality. In the future, this algorithm can be included as part of a standard foveated rendering scheme to improve the performance of virtual reality hardware.</span></p><p class="c1 c3"><span class="c9"></span></p><h1 class="c4" id="h.hz40cd8x1rw2"><span class="c10">REFERENCES</span></h1><p class="c1"><span class="c9">[1] Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V. Adve. 2021. ILLIXR: Enabling End-to-End Extended Reality Research. In 2021 IEEE International Symposium on Workload Characterization (IISWC). 24&ndash;38. https://doi.org/10.1109/IISWC53511.2021.00014</span></p><p class="c1"><span class="c9">[2] Ao Liu, Lirong Xia, Andrew Duchowski, Reynold Bailey, Kenneth Holmqvist, and Eakta Jain. 2019. Differential privacy for eye-tracking data. In Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications. 1&ndash;10.</span></p><p class="c1"><span class="c9">[3] Johannes Marinus Paulus Van Waveren. 2016. The asynchronous time warp for virtual reality on consumer hardware. In Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology. 37&ndash;46.</span></p><p class="c1 c3"><span class="c9"></span></p><p class="c1 c3"><span class="c9"></span></p></body></html>